[Unit]
Description=vLLM OpenAI-Compatible Server (GPU1 - RTX PRO 6000 96GB)
After=network.target

[Service]
Type=simple
User=vllm
Group=vllm

# GPU1 is your RTX PRO 6000 (96GB) per nvidia-smi
Environment=CUDA_DEVICE_ORDER=PCI_BUS_ID
Environment=CUDA_VISIBLE_DEVICES=1
Environment="TRANSFORMERS_CACHE=/opt/llm/hf"

# Optional but recommended: keep HF cache on /opt/llm
Environment=HF_HOME=/opt/llm/hf

WorkingDirectory=/opt/llm/vllm

ExecStart=/opt/llm/vllm/.venv/bin/vllm serve \
  --host 127.0.0.1 --port 8000 \
  --model /opt/llm/vllm/models/DeepSeek-R1-Distill-Qwen-32B \
  --served-model-name generator \
  --gpu-memory-utilization 0.92 \
  --max-model-len 65536 \
  --enforce-eager

Restart=always
RestartSec=2
LimitNOFILE=1048576

[Install]
WantedBy=multi-user.target

